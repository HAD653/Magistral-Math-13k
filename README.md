# GSM8K + OpenMath Magistral 13k

[![License: CC BY 4.0](https://img.shields.io/badge/License-CC_BY_4.0-brightgreen.svg)](https://creativecommons.org/licenses/by/4.0/)
![Task: Mathematical Reasoning](https://img.shields.io/badge/Task-Mathematical%20Reasoning-blue.svg)
![Format: Magistral](https://img.shields.io/badge/Format-Magistral-orange.svg)
![CoTs: GPT-OSS-120B](https://img.shields.io/badge/CoTs-GPT%E2%80%91OSS%E2%80%91120B-purple.svg)
![Language: English](https://img.shields.io/badge/Language-English-yellow.svg)
![Samples: 13,857](https://img.shields.io/badge/Samples-13,857-lightgrey.svg)

---

## Overview

**GSM8K + OpenMath Magistral 13k** is a curated math word-problem dataset for training **small language models** (‚âà0.5B‚Äì7B) on **chain-of-thought (CoT) reasoning**.

- **13,857** supervised examples  
- Grade-school to early high-school **math word problems**  
- Each sample has:
  - `question` ‚Äì natural language problem
  - `cot` ‚Äì full solution in a **Magistral** format:
    - `Problem:`  
    - `Reasoning:`  
    - `Answer:`
  - `final_answer` ‚Äì canonical numeric answer (string)
- CoTs generated by a strong open model (**GPT-OSS-120B**) and normalized

The dataset is meant to be **small-model-friendly**: focused on easy‚Äìmedium difficulty problems that are realistically solvable by 1B‚Äì3B parameter models.

---

## File layout

This repository contains:

```text
.
‚îú‚îÄ‚îÄ magistral_math_13k.jsonl   # main dataset file
‚îú‚îÄ‚îÄ README.md                  # this file
‚îî‚îÄ‚îÄ LICENSE                    # CC BY 4.0
````

The dataset is stored as a single **JSONL** file (`magistral_math_13k.jsonl`), one JSON object per line.

---

## Data format

Each line in `magistral_math_13k.jsonl` has the following fields:

* `question` *(string)*
  Natural language math word problem.

* `cot` *(string)*
  Full chain-of-thought solution, formatted as:

  ```text
  Problem:
  <restate the problem>

  Reasoning:
  <step-by-step reasoning>

  Answer:
  <final numeric answer>
  ```

* `final_answer` *(string)*
  Canonical final answer (usually a single integer), stored as text.

### Example

```json
{
  "question": "Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas and 2 small pizzas. A large pizza has 16 slices and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that day?",
  "cot": "Problem:\nAlbert buys 2 large pizzas (16 slices each) and 2 small pizzas (8 slices each). Find total slices eaten.\n\nReasoning:\n1. Large pizza slices = 2*16 = 32.\n2. Small pizza slices = 2*8 = 16.\n3. Total slices = 32+16 = 48.\n\nAnswer:\n48",
  "final_answer": "48"
}
```

---

## Loading the dataset

### With ü§ó `datasets` (local file)

```bash
pip install datasets
```

```python
from datasets import load_dataset

dataset = load_dataset(
    "json",
    data_files={"train": "magistral_math_13k.jsonl"},
)

print(dataset)
print(dataset["train"][0])
```

### With `pandas`

```bash
pip install pandas
```

```python
import pandas as pd

df = pd.read_json("magistral_math_13k.jsonl", lines=True)
print(df.head())
```

---

## Recommended training template

The dataset is designed for simple **Instruction / Response** fine-tuning.

### Input / output format

For SFT, a typical training sample can be built as:

```text
### Instruction:
{question}

### Response:
{cot}
```

Where:

* `{question}` comes from the `question` field,
* `{cot}` comes from the `cot` field.

### Example (Transformers)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "your-math-model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def format_example(example):
    prompt = f"### Instruction:\n{example['question']}\n\n### Response:\n{example['cot']}"
    return prompt

text = format_example(dataset["train"][0])
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
loss = outputs.loss  # if labels are set, etc.
```

At **inference time**, you usually only provide the question and let the model generate a fresh chain of thought:

```python
def format_prompt(question: str) -> str:
    return f"### Instruction:\n{question}\n\n### Response:\n"

prompt = format_prompt("Albert buys 2 large pizzas and 2 small pizzas...")
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_new_tokens=256,
    do_sample=False,  # greedy decoding for evaluation
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## Intended use & limitations

### Intended use

* Supervised fine-tuning of small language models (0.5B‚Äì7B) for:

  * **math word problems**
  * **step-by-step chain-of-thought reasoning**
* Evaluation / ablation studies on:

  * decoding strategies,
  * architectures (Transformer vs hybrid/SSM),
  * different CoT training regimes.

### Limitations

* Focuses on **easy‚Äìmedium** problems:

  * arithmetic, fractions, percentages,
  * basic algebra,
  * simple combinatorics and number problems.
* CoTs are **model-generated**, not human-authored:

  * mostly correct, but may contain occasional stylistic quirks or minor issues.
* Not meant for:

  * safety-critical applications,
  * high-stakes educational grading without human review.

Users are responsible for checking for any overlap with their evaluation sets (e.g., GSM8K test) to avoid data leakage.

---

## License

This dataset is released under the **Creative Commons Attribution 4.0 International (CC BY 4.0)** license.
See the `LICENSE` file or [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/) for details.

If you use this dataset in your work, please include an attribution such as:

> ‚ÄúWe use the *GSM8K + OpenMath Magistral 13k* dataset (HAD653, 2025), a collection of 13,857 math word problems with Magistral-style chain-of-thought solutions.‚Äù

---

## Citation

```bibtex
@dataset{had653_gsm8k_openmath_magistral_13k_2025,
  author       = {HAD653},
  title        = {GSM8K + OpenMath Magistral 13k: Chain-of-Thought Math Reasoning Dataset},
  year         = {2025},
  note         = {13,857 math word problems with Magistral-style chain-of-thought solutions.}
}
```

## Contact

For questions, issues or suggestions, please open a GitHub issue in this repository.
